{
  "hash": "27e571f9719192d90a4fd8785bb2900f",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"KMeans Clustering\"\nauthor: \"Nora Kristiansen og TorbjÃ¸rn Vatnelid\"\ndate: \"2024-04-15\"\ncategories: [llm]\nformat:\n  html:\n    code-overflow: wrap\n---\n\nWhen you reach a certain length of text to summarize, the other methods become too expensive or are not able to summarize well enough. We need to find a way to extract all the important parts of large texts like books or very big documents, and create a summary from them.\n\nSome simple ways to avoid using all the chunks for summarization is either randomly selecting chunks, or spacing out which chunks are selected. But what if we miss out on an important part of the text while doing this?\n\nA solution is K-means clustering, where each chunk is embedded, and then clusters are formed based on semantic meaning of those chunks. Then a summary is formed from each cluster, hopefully netting us a more accurate summary of huge texts.\n\nAnother advantage is the amount of requests sent to the API. While Map Reduction sends many requests, clustering will send only one request, saving a lot of money.\n\n### Loading our documents\nLet's implement this method to summarize many documents or whole books!\n\nFirst, we need to import some packages and load in our OpenAI API key, since we will be using OpenAI's GPT models.\n\n::: {#4e66fb23 .cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\"}\nfrom dotenv import load_dotenv\nfrom utils import read_files, split_document_by_tokens\nfrom pathlib import Path\nimport os\n\nload_dotenv()\nOPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n```\n:::\n\n\nNext, we will load in our documents. Let's load in a whole tender competition and a whole book (Pride and Prejudice).\n\n::: {#1d08adee .cell execution_count=2}\n``` {.python .cell-code code-fold=\"false\"}\nanbudskonkurranse = read_files(Path('./content/nord-universitet'))\nanbudskonkurranse_filnavn = [doc.metadata[\"source\"] for doc in anbudskonkurranse]\nprint(\"Tender competition documents:\\n\")\nfor konk in anbudskonkurranse_filnavn:\n    print(konk)\n\nbooks = read_files(Path(\"./content/books/\"))\npride_and_prejudice = books[0]\nprint(\"\\nBook:\\n\")\nprint(pride_and_prejudice.metadata[\"source\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTender competition documents:\n\ncontent/nord-universitet/Vedlegg_4_Profilhandbok-Nord-universitet.pdf\ncontent/nord-universitet/Vedlegg_5.B_databehandleravtale_bilag_2020.docx\ncontent/nord-universitet/Vedlegg_3_CMS-anbefaling-Leveranse-Nord-universitet.pdf\ncontent/nord-universitet/Vedlegg_2_Analyse-og-evaluering-Leveranse-Nord-universitet.pdf\ncontent/nord-universitet/ssa-v_bilag_2018_bok (1).docx\ncontent/nord-universitet/ssa-v_generell_avtaletekst_2018_bok (2).docx\ncontent/nord-universitet/ssa-t-bilag-2.docx\ncontent/nord-universitet/Vedlegg_1_FS-integrasjon.pdf\ncontent/nord-universitet/Vedlegg_5.A_dba_generell_avtaletekst_2020_no.docx\ncontent/nord-universitet/Vedlegg_6_Retningslinjer-for-www-nord-no-vedtatt-februar-2021.pdf\ncontent/nord-universitet/ssa-t_generell-avtaletekst-2018-bok.docx\ncontent/nord-universitet/ssa-v-bilag-2.docx\ncontent/nord-universitet/ssa-t_bilag_2018_bok.docx\ncontent/nord-universitet/Vedlegg_7_Prisskjema.xlsx\ncontent/nord-universitet/KONKURRANSEGRUNNLAG.docx\n\nBook:\n\ncontent/books/pride_and_prejudice.pdf\n```\n:::\n:::\n\n\nLet's see how many tokens are in our documents!\n\n::: {#da550b39 .cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\"}\nfrom langchain_openai import ChatOpenAI\nfrom langchain.schema import Document\n\nllm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\")\n\ntender_content = \"\"\n\nfor doc in anbudskonkurranse:\n    tender_content += doc.page_content\n\ntender_documents = Document(page_content=tender_content)\n\nnum_tokens_tender = llm.get_num_tokens(tender_documents.page_content)\nprint(f\"Number of tokens in tender competition documents: {num_tokens_tender}\")\n\nnum_tokens_book = llm.get_num_tokens(pride_and_prejudice.page_content)\nprint(f\"Number of tokens in book: {num_tokens_book}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of tokens in tender competition documents: 137133\nNumber of tokens in book: 160831\n```\n:::\n:::\n\n\nThat's a lot of tokens. If we were to use map reduction, sending all these tokens to the LLM would be pretty expensive. If we were using document stuffing, we might not be able to fit the document(s) at all!\n\n### Chunking our documents\n\nTo better facilitate clustering, let's split our documents into more manageable sizes, called chunks. There are many different ways to chunk a document, the easiest being to split on specific characters, like punctuation marks. Another method is to chunk on token count, this can be nice because you know how many tokens you're sending to the LLM with each chunk.\n\nLet's try with token chunking.\n\n::: {#dadc2ed9 .cell execution_count=4}\n``` {.python .cell-code code-fold=\"false\"}\nsplit_tender_competition = split_document_by_tokens(anbudskonkurranse, chunk_size=2000, overlap=200)\n\nsplit_book = split_document_by_tokens([pride_and_prejudice], chunk_size=2000, overlap=200)\n\nprint(f\"Now our tender competition is split up into {len(split_tender_competition)} documents\")\n\nprint(f\"And our book is split up into {len(split_book)} documents\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNow our tender competition is split up into 98 documents\nAnd our book is split up into 92 documents\n```\n:::\n:::\n\n\n### Embedding\n\nClustering relies on embeddings to work. Embeddings are vector representations of text, so that LLMs can work with them (LLMs don't understand human readable text, they understand numbers). Similar pieces of text will be closer together in the vector space, therefore the hope is that we can \"cluster\" pieces of text with similar meaning together, because they are closer together in the vector space.\n\n::: {#ac2dfbb6 .cell execution_count=5}\n``` {.python .cell-code}\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, model=\"text-embedding-3-small\")\n\ntender_vectors = embeddings.embed_documents([doc.page_content for doc in split_tender_competition])\n\nbook_vectors = embeddings.embed_documents([doc.page_content for doc in split_book])\n```\n:::\n\n\n::: {#8b5290ec .cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\"}\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_clusters(vectors, title, algo = None, ):\n    tsne = TSNE(n_components=2, random_state=42)\n    vectors = np.array(vectors)\n    reduced_data_tsne = tsne.fit_transform(vectors)\n    if algo:\n        plt.scatter(reduced_data_tsne[:, 0], reduced_data_tsne[:, 1], c=algo.labels_)\n    else:\n        plt.scatter(reduced_data_tsne[:, 0], reduced_data_tsne[:, 1])\n    plt.xlabel('Dimension 1')\n    plt.ylabel('Dimension 2')\n    plt.title(f'Cluster visualization for {title}.')\n    plt.show()\n```\n:::\n\n\nLet's try to visualize how our chunks look in the vector space. Please note that the embedding vectors have 1536 dimensions, and here we have squished them down to two dimension, so a lot of information will be lost. Perhaps some clustering of chunks would be easier to see with more dimensions.\n\n::: {#e22d68b4 .cell execution_count=7}\n``` {.python .cell-code}\nplot_clusters(vectors=book_vectors, title=\"Pride and Prejudice\")\nplot_clusters(vectors=tender_vectors, title=\"the tender competition\")\n```\n\n::: {.cell-output .cell-output-display}\n![](clustering_files/figure-html/cell-8-output-1.png){width=587 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![](clustering_files/figure-html/cell-8-output-2.png){width=587 height=449}\n:::\n:::\n\n\n### Clustering\n\nK-means clustering is an algorithm which starts by randomly initializing the centroids of a given amount of clusters (k). A centroid is one datapoint in the center of a cluster. It then iterates through two main steps: assignment and update. \n\nIn the assignment step, each data point is assigned to the closest centroid based on a distance metric, typically Euclidean distance. In the update step, the centroids are recalculated as the mean of all data points assigned to each cluster. \n\nThis process repeats until the centroids no longer move significantly, indicating convergence. The result is a grouping of data points such that points in the same cluster are more similar to each other than to those in other clusters, based on the chosen distance metric.\n\nNote that K-means takes the amount of clusters as a parameter. For now, we'll set it manually.\n\n::: {#f65e098e .cell execution_count=8}\n``` {.python .cell-code}\nfrom sklearn.cluster import KMeans\n\nnum_clusters = 11\n\ntender_kmeans = KMeans(n_clusters=num_clusters, random_state=42).fit(tender_vectors)\n\nbook_kmeans = KMeans(n_clusters=num_clusters, random_state=42).fit(book_vectors)\n```\n:::\n\n\nNow let's visualize our clusters:\n\n::: {#a9e1a334 .cell execution_count=9}\n``` {.python .cell-code code-fold=\"true\"}\nplot_clusters(vectors=tender_vectors, algo=tender_kmeans, title=\"the tender competition with 11 clusters\")\nplot_clusters(vectors=book_vectors, algo=book_kmeans, title=\"Pride and Prejudice with 11 clusters\")\n```\n\n::: {.cell-output .cell-output-display}\n![](clustering_files/figure-html/cell-10-output-1.png){width=587 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![](clustering_files/figure-html/cell-10-output-2.png){width=587 height=449}\n:::\n:::\n\n\nThis looks alright, especially the tender competition has some clear clusters. However, how can we know that we have chosen the correct amount of clusters for our K-means algorithm?\n\n### Silhouette scoring\n\nSilhouette scoring is a method used to assess the quality of clusters when using for example K-means. The silhouette score for each data point is calculated based on two factors: \n- The average distance between the data point and all other points in the same cluster (cohesion)\n- The average distance between the data point and all points in the nearest cluster to which the data point does not belong (separation). \n\nWe find the silhouette score for one data point by taking the difference between these two distances, normalized by the maximum of the two. This score ranges from -1 to 1, where a score close to 1 indicates that the data point clearly belongs to its own cluster, a score near 0 indicates that the point is on the border of two clusters, and a score near -1 suggests that the point may have been assigned to the wrong cluster. The overall silhouette score of the clustering is the average of the silhouette scores of all the data points.\n\nSo what we can do is try many different values of k and find the silhouette score for each one, then choose the best scoring k as our cluster amount, where the score closest to 1 is best.\n\n::: {#35768891 .cell execution_count=10}\n``` {.python .cell-code}\nfrom sklearn.metrics import silhouette_score\n\ndef find_optimal_clusters(vectors, max_k):\n    score = -1\n    best_n_clusters = 0\n    for k in range(3, max_k):\n        kmeans = KMeans(n_clusters=k, random_state=1).fit_predict(vectors)\n        new_score = silhouette_score(vectors, kmeans)\n        if new_score > score:\n            best_n_clusters = k\n            score = new_score\n        print(f\"For n_clusters = {k}, silhouette score is {new_score})\")\n    print(f\"Best number of clusters is {best_n_clusters}\")\n    return best_n_clusters\n```\n:::\n\n\nLet's find the optimal number of cluster for our tender competition:\n\n::: {#3aae19dc .cell execution_count=11}\n``` {.python .cell-code}\ntender_num_clusters = find_optimal_clusters(tender_vectors, max_k=20)\ntender_kmeans = KMeans(n_clusters=tender_num_clusters, random_state=1).fit(tender_vectors)\nplot_clusters(vectors=tender_vectors, algo=tender_kmeans, title=f\"our tender competition with {tender_num_clusters} clusters\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFor n_clusters = 3, silhouette score is 0.16319734949495834)\nFor n_clusters = 4, silhouette score is 0.13677655311745174)\nFor n_clusters = 5, silhouette score is 0.13867432915900288)\nFor n_clusters = 6, silhouette score is 0.11354961404064279)\nFor n_clusters = 7, silhouette score is 0.10669988485818448)\nFor n_clusters = 8, silhouette score is 0.10039881578450706)\nFor n_clusters = 9, silhouette score is 0.0775684824083643)\nFor n_clusters = 10, silhouette score is 0.07354397576934713)\nFor n_clusters = 11, silhouette score is 0.0896860457734401)\nFor n_clusters = 12, silhouette score is 0.09227585159583783)\nFor n_clusters = 13, silhouette score is 0.09388758282715984)\nFor n_clusters = 14, silhouette score is 0.09267778721302193)\nFor n_clusters = 15, silhouette score is 0.09460706544627888)\nFor n_clusters = 16, silhouette score is 0.0869772390044283)\nFor n_clusters = 17, silhouette score is 0.09299719039273895)\nFor n_clusters = 18, silhouette score is 0.10369596708641589)\nFor n_clusters = 19, silhouette score is 0.10545737216311063)\nBest number of clusters is 3\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](clustering_files/figure-html/cell-12-output-2.png){width=587 height=449}\n:::\n:::\n\n\nAnd then for our book:\n\n::: {#7cc93c4f .cell execution_count=12}\n``` {.python .cell-code code-fold=\"true\"}\nbook_num_clusters = find_optimal_clusters(book_vectors, max_k=20)\nbook_kmeans = KMeans(n_clusters=book_num_clusters, random_state=1).fit(book_vectors)\nplot_clusters(vectors=book_vectors, algo=book_kmeans, title=f\"Pride and Prejudice with {book_num_clusters} clusters\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFor n_clusters = 3, silhouette score is 0.047733092427202854)\nFor n_clusters = 4, silhouette score is 0.051067859191365084)\nFor n_clusters = 5, silhouette score is 0.05523242135225988)\nFor n_clusters = 6, silhouette score is 0.0465462151274959)\nFor n_clusters = 7, silhouette score is 0.030455545138270337)\nFor n_clusters = 8, silhouette score is 0.03968968584609667)\nFor n_clusters = 9, silhouette score is 0.03900376290198097)\nFor n_clusters = 10, silhouette score is 0.0393203646925307)\nFor n_clusters = 11, silhouette score is 0.041871536535647465)\nFor n_clusters = 12, silhouette score is 0.04602273803755732)\nFor n_clusters = 13, silhouette score is 0.04664794070136)\nFor n_clusters = 14, silhouette score is 0.04450307354685479)\nFor n_clusters = 15, silhouette score is 0.037468937738576666)\nFor n_clusters = 16, silhouette score is 0.036899065375207624)\nFor n_clusters = 17, silhouette score is 0.04084609100393472)\nFor n_clusters = 18, silhouette score is 0.04017819773375103)\nFor n_clusters = 19, silhouette score is 0.039037963267425264)\nBest number of clusters is 5\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](clustering_files/figure-html/cell-13-output-2.png){width=587 height=449}\n:::\n:::\n\n\n### Summarization using K-means\n\nNow for the final piece of the puzzle. We have the clusters, now what? Let's find the chunk which sits closest to the center of each cluster.\n\n::: {#31b92e07 .cell execution_count=13}\n``` {.python .cell-code}\ndef get_key_chunks(vectors, alg, num_clusters, documents):\n    closest_indices = []\n\n    for i in range(num_clusters):\n        distances = np.linalg.norm(vectors - alg.cluster_centers_[i], axis=1)\n\n        closest_index = np.argmin(distances)\n        closest_indices.append(closest_index)\n\n    selected_indices = sorted(closest_indices)\n    selected_docs = [documents[doc] for doc in selected_indices]\n    return selected_docs\n```\n:::\n\n\nThe centroid chunks will be used as the \"average\" chunk for each cluster, hopefully giving us a solid idea of what the entire cluster is talking about.\n\nNow that we have the chunks, let's do some good old map reduction on them:\n\n::: {#ede73619 .cell execution_count=14}\n```` {.python .cell-code}\nfrom langchain import PromptTemplate\nfrom langchain.chains.summarize import load_summarize_chain\n\nllm4 = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-4-turbo\")\n\nmap_prompt = \"\"\"\nYou will be given a piece of a larger text. This piece of text will be enclosed in triple backticks (```).\nYour job is to give a summary of this piece of text so that the reader will have a full understanding of what the text is about.\nYour response should be at least three paragraphs and fully encompass what was written in the piece of text.\n\n```{text}```\n\nFULL SUMMARY:\n\"\"\"\n\nmap_prompt_template = PromptTemplate(template=map_prompt, input_variables=[\"text\"])\n\nmap_chain = load_summarize_chain(llm=llm, chain_type=\"stuff\", prompt=map_prompt_template)\n````\n:::\n\n\nWe once again start with the tender competition:\n\n::: {#6c683572 .cell execution_count=15}\n``` {.python .cell-code}\nsummary_list = []\n\nselected_tender_docs = get_key_chunks(tender_vectors, tender_kmeans, tender_num_clusters, split_tender_competition)\n\nfor i, doc in enumerate(selected_tender_docs):\n    chunk_summary = map_chain.run([doc])\n    summary_list.append(chunk_summary)\n\n    print(f'Summary #{i} - Preview: {chunk_summary[:250]} \\n')\n\ntender_summaries = \"\\n\".join(summary_list)\n\ntender_summaries = Document(page_content=tender_summaries)\n\nprint(f'All your summaries together are {llm.get_num_tokens(tender_summaries.page_content)} tokens')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSummary #0 - Preview: The text discusses the need for the university's website to be more visually appealing and user-friendly while still maintaining a sense of creativity. It emphasizes the importance of standing out among other universities by daring to have a clear pe \n\nSummary #1 - Preview: The piece of text provided outlines various important clauses and terms within a contractual agreement. It emphasizes the importance of confidentiality, stating that both parties must maintain confidentiality even after the agreement has ended. Emplo \n\nSummary #2 - Preview: The piece of text provided outlines the requirements for a Content Management System (CMS) solution for Nord University. It emphasizes the importance of having a solution that can efficiently manage broken links, redirects, user-friendly interfaces,  \n\nAll your summaries together are 801 tokens\n```\n:::\n:::\n\n\nAnd then, finally, we combine the summaries.\n\n::: {#cef54639 .cell execution_count=16}\n```` {.python .cell-code}\ncombine_prompt = \"\"\"\nYou will now be given a series of summaries from a larger text. The summaries will be enclosed in triple backticks(```).\nYour goal is to give a summary of what happened in the greater piece of text.\nThe reader should be able to grasp what the full text is about from your summary.\n\n```{text}```\nSUMMARY:\n\"\"\"\n\ncombine_prompt_template = PromptTemplate(template=combine_prompt, input_variables=[\"text\"])\n\nreduce_chain = load_summarize_chain(llm=llm4, chain_type=\"stuff\", prompt=combine_prompt_template)\n````\n:::\n\n\n::: {#0752073b .cell execution_count=17}\n``` {.python .cell-code}\nimport textwrap\ntender_output = reduce_chain.run([tender_summaries])\ntextwrap.wrap(text=tender_output, width=100, replace_whitespace=False)\n```\n\n::: {.cell-output .cell-output-display execution_count=34}\n```\n[\"The larger text primarily discusses the enhancement and management of Nord University's digital\",\n 'presence, focusing on three key areas: website design and user experience, contractual obligations',\n 'and terms for services, and specifications for a Content Management System (CMS). \\n\\nFirstly, the',\n \"text outlines the importance of redesigning the university's website to be more visually appealing,\",\n \"user-friendly, and accessible, while emphasizing the institution's unique qualities and research\",\n 'activities. It suggests improvements such as simplifying language, using engaging content, and',\n 'ensuring universal design to attract potential students and researchers.\\n\\nSecondly, the text delves',\n 'into detailed contractual terms and conditions between parties, stressing the significance of',\n 'confidentiality, compensation specifics, payment terms, legal compliance, and security measures.',\n 'This section provides a comprehensive overview of the expectations and responsibilities to ensure',\n 'smooth contractual relationships.\\n\\nLastly, the text specifies requirements for a CMS solution',\n 'tailored for Nord University. It emphasizes functionalities like managing broken links, user',\n 'authentication, profile management, workflow processes, and document accessibility to enhance',\n \"content management and user experience on the university's digital platforms.\\n\\nOverall, the text\",\n \"serves as a guide to improving and managing various aspects of Nord University's digital strategy to\",\n 'better serve its community and stakeholders.']\n```\n:::\n:::\n\n\nLastly, let's repeat that process for Pride and Prejudice:\n\n::: {#b41b40b2 .cell execution_count=18}\n``` {.python .cell-code code-fold=\"true\"}\nsummary_list = []\n\nselected_book_docs = get_key_chunks(book_vectors, book_kmeans, book_num_clusters, split_book)\n\nfor i, doc in enumerate(selected_book_docs):\n    chunk_summary = map_chain.run([doc])\n    summary_list.append(chunk_summary)\n\n    print(f'Summary #{i} - Preview: {chunk_summary[:250]} \\n')\n\nbook_summaries = \"\\n\".join(summary_list)\n\nbook_summaries = Document(page_content=book_summaries)\n\nprint(f'All your summaries together are {llm.get_num_tokens(book_summaries.page_content)} tokens')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSummary #0 - Preview: The piece of text revolves around a discussion between Elizabeth and another character (likely Jane) regarding Mr. Bingley's behavior and relationships. Elizabeth expresses her belief that Mr. Bingley's actions are not driven by design but rather by  \n\nSummary #1 - Preview: In this piece of text, the characters Elizabeth, Mrs. Gardiner, and others are discussing Mr. Darcy's behavior and character. Mrs. Gardiner expresses surprise at Darcy's treatment of Mr. Wickham, noting his pleasant appearance but questioning his act \n\nSummary #2 - Preview: The enclosed text depicts a conversation between Elizabeth and her family regarding the elopement of Lydia with Wickham. Elizabeth expresses concerns about Lydia's lack of moral values and upbringing, attributing her actions to her youth, frivolous l \n\nSummary #3 - Preview: In this piece of text, we see Elizabeth Bennet feeling disappointed and hurt by Mr. Darcy's behavior when they meet again after some time. Despite her attempts to engage him in conversation, Mr. Darcy remains silent and distant, causing Elizabeth to  \n\nSummary #4 - Preview: In this piece of text, we see a scene where Jane Bennet's engagement to Mr. Bingley is announced and celebrated by her family. Mrs. Bennet is ecstatic and expresses her happiness at the match, believing Jane to be the luckiest and most beautiful of h \n\nAll your summaries together are 1593 tokens\n```\n:::\n:::\n\n\n::: {#b4c50f82 .cell execution_count=19}\n``` {.python .cell-code code-fold=\"true\"}\nbook_output = reduce_chain.run([book_summaries])\ntextwrap.wrap(text=book_output, width=100, replace_whitespace=False)\n```\n\n::: {.cell-output .cell-output-display execution_count=36}\n```\n['The overarching narrative encapsulated in the text pieces revolves around the intricate social',\n 'dynamics and emotional developments primarily within the Bennet family from Jane Austen\\'s \"Pride and',\n 'Prejudice.\" The story threads highlight delicate conversations and evolving relationships influenced',\n 'by societal norms, romantic entanglements, and personal values.\\n\\nCentral to the narrative is',\n 'Elizabeth Bennet, whose astute observations and interactions with characters like Mr. Bingley, Mr.',\n 'Darcy, and her own family members drive much of the plot. Discussions often revolve around the',\n \"complexities of relationships, such as Mr. Bingley's fluctuating attentions and Mr. Darcy's\",\n 'seemingly aloof but complex disposition, which Elizabeth tries to decipher. Key events, such as Mr.',\n \"Bingley's behavior towards Jane Bennet, Lydia Bennet's reckless elopement with Mr. Wickham, and the\",\n 'unexpected visit from Lady Catherine de Bourgh, catalyze shifts in these relationships and societal',\n \"perceptions.\\n\\nThe narrative captures the tension and excitement surrounding Jane Bennet's engagement\",\n \"to Mr. Bingley, juxtaposed with Elizabeth's personal turmoil in understanding her feelings towards\",\n 'Mr. Darcy amidst societal and familial expectations. The text also explores themes of morality,',\n 'reputation, and personal growth as characters navigate the challenges posed by their desires,',\n 'obligations, and the prevailing social etiquette.\\n\\nOverall, the text pieces collectively portray the',\n 'emotional landscape and social intricacies of early 19th-century England through the lens of the',\n \"Bennet family's experiences, focusing on romance, social status, and personal integrity.\"]\n```\n:::\n:::\n\n\nAnd that's it!\n\n",
    "supporting": [
      "clustering_files"
    ],
    "filters": [],
    "includes": {}
  }
}